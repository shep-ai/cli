# Research Artifact (YAML)
# This is the source of truth. Markdown is auto-generated from this file.

name: realtime-feature-status
summary: >
  Technical research for building the real-time agent event streaming infrastructure.
  Key decisions: DB-polling SSE route (avoids Turbopack singleton issues), Service Worker
  for tab multiplexing, globalThis-based EventEmitter singleton, NotificationWatcherService
  for status/phase change detection, and exponential backoff reconnection.

relatedFeatures: []

technologies:
  - 'Node.js EventEmitter (typed via generics)'
  - 'Server-Sent Events (ReadableStream + TextEncoder)'
  - 'Service Workers (postMessage broadcast)'
  - 'React 19 (useState, useEffect, useRef, useCallback)'
  - 'Next.js 16+ (route handlers, force-dynamic)'
  - 'tsyringe (DI for repository injection)'
  - 'Vitest (unit + integration testing)'

relatedLinks: []

decisions:
  - title: 'SSE route implementation strategy'
    chosen: 'DB-polling with per-connection cache inside the route handler'
    rejected:
      - >
        In-process event bus subscription in route handler — Next.js Turbopack bundles
        route handlers into separate module contexts. An EventEmitter singleton created
        in the dev-server process is invisible to the route handler's module scope.
        This was the original approach and it silently produced zero events.
      - >
        WebSocket instead of SSE — Over-engineered for unidirectional server→client
        events. SSE is simpler, works with HTTP/2 multiplexing, auto-reconnects natively,
        and requires no additional server infrastructure.
    rationale: >
      The route handler polls the database directly (features + agent_runs + phase_timings)
      every 500ms using a per-connection cache (Map<featureId, CachedFeatureState>). On each
      poll, it compares current state against cached state and emits only deltas. This avoids
      the cross-module singleton problem entirely — the route handler is self-contained with
      no dependency on shared in-process state. The cache seeds on first observation (no
      initial event burst) and tracks status + completedPhases per feature.

  - title: 'Service Worker for SSE connection multiplexing'
    chosen: 'Dedicated Service Worker (agent-events-sw.js) that maintains one SSE connection and broadcasts to all tabs'
    rejected:
      - >
        SharedWorker — Less browser support (no Safari on iOS). Service Workers have
        broader support and also survive page navigations, providing better connection
        persistence.
      - >
        One EventSource per tab — Wasteful: N tabs = N SSE connections = N×polling load
        on the server. The SSE route polls the DB on each connection independently.
    rationale: >
      The Service Worker subscribes/unsubscribes based on client messages. It opens the
      SSE connection on first subscriber and closes it when the last subscriber leaves.
      Events are broadcast to all tabs via clients.matchAll(). Connection status (connected,
      connecting, disconnected) is also broadcast so the UI can show connection indicators.
      The SW handles its own reconnection with exponential backoff.

  - title: 'NotificationBus singleton strategy'
    chosen: 'globalThis with Symbol.for() key'
    rejected:
      - >
        Module-level singleton (let bus: EventEmitter) — Fails in Next.js where Turbopack
        may create multiple copies of the module. Each copy gets its own singleton.
      - >
        Dependency injection via tsyringe — The bus needs to be accessible from both
        bundled route handlers and the dev-server process. DI container resolution may
        not cross module boundaries in the same way.
    rationale: >
      Symbol.for('shep:notification-bus') creates a global, cross-module-boundary key on
      globalThis. This survives Turbopack's module isolation because globalThis is shared
      across all JS execution contexts in the same process. The bus is lazily created on
      first access. A resetNotificationBus() function is provided for test isolation.

  - title: 'NotificationWatcherService polling approach'
    chosen: 'Poll agent_runs table, track per-run state, emit via INotificationService'
    rejected:
      - >
        File system watching (e.g., chokidar on SQLite file) — SQLite WAL mode makes
        file change detection unreliable. Would need to re-query anyway after detecting
        a change.
      - >
        IPC from worker process — The feature agent worker is spawned detached with IPC
        disconnected (for background execution). No IPC channel available.
    rationale: >
      The watcher polls every 3 seconds (configurable), listing all agent runs. It tracks
      active runs (pending, running, waiting_approval) in a Map<runId, WatcherState>.
      Status changes emit mapped NotificationEvents. Phase completions are detected by
      comparing phase_timings.completedAt against the cached completedPhases set. Terminal
      runs (completed, failed, cancelled, interrupted) are emitted once then cleaned up.
      Feature names are resolved via IFeatureRepository.

  - title: 'React hook architecture'
    chosen: 'Service Worker primary with direct EventSource fallback'
    rejected:
      - >
        EventSource only — Works but creates redundant connections when multiple tabs
        are open. No connection sharing possible with raw EventSource.
      - >
        Polling from React (useEffect + fetch) — Higher latency, no streaming, wastes
        bandwidth re-fetching unchanged data.
    rationale: >
      useAgentEvents() first attempts to register a Service Worker. If SW is available,
      events arrive via navigator.serviceWorker.onmessage. If SW registration fails
      (e.g., HTTPS requirement, browser restriction), it falls back to a direct
      EventSource connection with the same backoff/reconnection logic. Both paths
      provide the same { events, lastEvent, connectionStatus } interface.

openQuestions: []

content: |
  ## Technology Decisions

  ### 1. DB-Polling SSE Route (not event bus subscription)

  **Chosen:** Route handler polls DB directly with per-connection cache

  **Rejected:**
  - In-process event bus subscription — Fails due to Turbopack module isolation
  - WebSocket — Over-engineered for unidirectional events

  **Rationale:** Self-contained route handler with no cross-module dependencies. Polls
  every 500ms, compares against cached state, emits only deltas. Seeds cache on first
  observation to avoid initial event burst.

  ### 2. Service Worker for Tab Multiplexing

  **Chosen:** Dedicated SW that broadcasts SSE events to all tabs

  **Rejected:**
  - SharedWorker — Less browser support (no Safari iOS)
  - One EventSource per tab — Wasteful redundant connections

  **Rationale:** One SSE connection per browser, shared via postMessage. Opens on first
  subscriber, closes on last unsubscribe. Handles reconnection with exponential backoff.

  ### 3. globalThis Singleton for NotificationBus

  **Chosen:** Symbol.for() key on globalThis

  **Rejected:**
  - Module-level singleton — Breaks with Turbopack module duplication
  - DI container — May not cross module boundaries

  **Rationale:** globalThis is shared across all JS contexts in a process. Symbol.for()
  prevents key collisions. Reset function provided for tests.

  ### 4. NotificationWatcherService for Status Detection

  **Chosen:** Poll agent_runs table, track per-run state

  **Rejected:**
  - File system watching — Unreliable with SQLite WAL
  - IPC from worker — Worker spawned detached, no IPC channel

  **Rationale:** 3-second polling with in-memory state tracking. Detects status changes
  and phase completions. Emits through INotificationService for fan-out.

  ## Architecture Notes

  ### Event Flow

  ```
  Agent worker updates DB (agent_runs, phase_timings)
    → NotificationWatcherService polls DB every 3s
    → Detects status change or phase completion
    → Emits NotificationEvent via INotificationService
    → NotificationBus fans out to listeners

  Meanwhile (independent path for web UI):
    SSE route polls DB every 500ms
    → Per-connection cache detects deltas
    → Streams NotificationEvent via SSE
    → Service Worker receives event
    → Broadcasts to all open tabs
    → useAgentEvents hook delivers to React components
  ```

  ### Status-to-Event Mapping

  | AgentRunStatus    | NotificationEventType | NotificationSeverity |
  | ----------------- | --------------------- | -------------------- |
  | running           | AgentStarted          | Info                 |
  | waitingApproval   | WaitingApproval       | Warning              |
  | completed         | AgentCompleted        | Success              |
  | failed            | AgentFailed           | Error                |

  ### Files Changed

  | File | Change | Type |
  | ---- | ------ | ---- |
  | `packages/core/src/infrastructure/services/notifications/notification-bus.ts` | New typed EventEmitter singleton | New |
  | `packages/core/src/infrastructure/services/notifications/notification-watcher.service.ts` | New DB-polling status detector | New |
  | `src/presentation/web/app/api/agent-events/route.ts` | Rewritten: DB polling with per-connection cache | Rewrite |
  | `src/presentation/web/public/agent-events-sw.js` | New Service Worker for SSE multiplexing | New |
  | `src/presentation/web/hooks/use-agent-events.ts` | Rewritten: SW-first with EventSource fallback | Rewrite |
  | `src/presentation/web/dev-server.ts` | Initialize notification watcher on startup | Modified |
  | `src/presentation/cli/commands/ui.command.ts` | Initialize notification watcher on startup | Modified |

  ## Security Considerations

  - SSE endpoint is read-only (GET), no mutation surface
  - No authentication changes (inherits existing session model)
  - Service Worker scoped to '/' — same-origin only
  - No user input in DB queries (repository pattern with parameterized queries)

  ## Performance Implications

  - SSE route polls every 500ms per connection — acceptable for single-user tool
  - Service Worker reduces N tab connections to 1
  - NotificationWatcherService polls every 3s — minimal DB load
  - Per-connection cache ensures only deltas are serialized and sent
